all the experiments have been done in the chargpt folder, with the gpt-micro (for faster execution [time]s)

the models in chargpt have been trained with tiny-shakespear (1.1 mb)

trained with 20000 iterations

simple transformer => 0.83M total parameters
best train loss after all iterations => 1.25

transformer with ltm => 0.83M total parameters
best train loss at after all iterations => 1.17

transformer with ltm is 6.4% better than simple transformer
